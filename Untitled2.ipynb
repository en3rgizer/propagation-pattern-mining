{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e8bf1-2d91-4ffe-acc3-c7f20cd654fd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from conf import *\n",
    "import random\n",
    "import pyspark\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from subprocess import check_output\n",
    "from sys import exit\n",
    "import subprocess\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark import StorageLevel\n",
    "from operator import add\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from interval_tree import Interval, Node, IntervalTree\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "from pathlib import Path\n",
    "from event import event\n",
    "from node2 import node2\n",
    "\n",
    "class node:\n",
    "    type = ''\n",
    "    refined_type = ''\n",
    "    zip = ''\n",
    "    childs = []\n",
    "    parents = []\n",
    "    visited = False\n",
    "    def __init__(self, type, refined_type, zip, childs, parents):\n",
    "        self.type         = type\n",
    "        self.refined_type = refined_type\n",
    "        self.zip          = zip\n",
    "        self.childs       = (list(childs) if len(childs)!= 0 else [])\n",
    "        self.parents      = (list(parents) if len(parents)!= 0 else [])\n",
    "        self.visited      = False\n",
    "\n",
    "\n",
    "def event_transformer(line: str) -> event:\n",
    "    r = line.replace('\\r', '').replace('\\n', '').split(',')\n",
    "    childs = set()\n",
    "    parents = set()\n",
    "    if len(r[16])>0:\n",
    "        childs = set(r[16].split(';'))\n",
    "    if len(r[17])>0:\n",
    "        parents = set(r[17].split(';'))\n",
    "            \n",
    "    if r[1] == 'W':\n",
    "        e = event(eventId=r[0], type='W', refinedType=r[2], startTime=r[3], endTime=r[4], locationLat=0, locationLng=0,  \n",
    "                          distance=0, airportCode=r[8], number=0, street='NA', side='NA', city='NA', county='NA', state='NA', \n",
    "                          zipCode='NA', childs=childs, parents=parents)\n",
    "    else:        \n",
    "        e = event(eventId=r[0], type='T', refinedType=r[2], startTime=r[3], endTime=r[4], locationLat=float(r[5]), \n",
    "                          locationLng=float(r[6]), distance=float(r[7]), airportCode=r[15], number=(0 if r[9]=='N/A' or r[9]=='' else int(r[9])), \n",
    "                          street=r[10], side=r[11], city=r[12], county=r[13], state=r[14], zipCode=r[15], childs=childs, parents=parents)\n",
    "    return e\n",
    "\n",
    "def str_transformer(e: event) -> str:\n",
    "    child, parents = list(e.childs), list(e.parents)\n",
    "    child_str = ';'.join(child) if len(child) > 0 else ''\n",
    "    parent_str = ';'.join(parents) if len(parents) > 0 else ''\n",
    "    if e.type == 'W':\n",
    "        res = f'{e.eventId},{e.type},{e.refinedType},{e.startTime},{e.endTime},N/A,N/A,N/A,{e.airportCode},N/A,N/A,N/A,N/A,N/A,N/A,N/A,{child_str},{parent_str}'\n",
    "\n",
    "    else:\n",
    "        res = f'{e.eventId},{e.type},{e.refinedType},{e.startTime},{e.endTime},{e.locationLat},{e.locationLng},{e.distance},{e.airportCode},{e.number},{e.street},{e.side},{e.city},{e.county},{e.state},{e.zipCode},{child_str},{parent_str}'\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "sc.stop()\n",
    "# Create new config\n",
    "conf = pyspark.SparkConf().setAll([(\"spark.driver.maxResultSize\", '16g'), ('spark.executor.memoryOverhead', '16g'), ('spark.executor.memory', '16g')])\n",
    "\n",
    "SparkSession.builder.config(conf=conf)\n",
    "spark = SparkSession.builder.appName('test_05_11_1').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.addPyFile('file:/event.py')\n",
    "sc.addPyFile('file:/node2.py')\n",
    "\n",
    "storage_level = pyspark.StorageLevel.MEMORY_AND_DISK\n",
    "sc.setLogLevel(\"OFF\")\n",
    "\n",
    "#print(spark.sparkContext.getConf().getAll())\n",
    "\n",
    "if SUBSET:\n",
    "    f_name = f'_{CITY}_TTR-{trTimeThresh}'\n",
    "    res = input(f'You are using a SUBSET ({CITY}) with trTimeThresh = {trTimeThresh}. Are you sure? [y/n]:')\n",
    "else:\n",
    "    res = input(f'You are using ENTIRE DATASET with trTimeThresh = {trTimeThresh}. Are you sure? [y/n]:')\n",
    "    f_name = f'_TTR-{trTimeThresh}'\n",
    "\n",
    "if(res != 'y'):\n",
    "    exit()\n",
    "\n",
    "start = time.time()    \n",
    "\n",
    "traffic_path = f'file:/datasets/TrafficEvents{f_name}.csv'\n",
    "weather_path = f'file:/datasets/WeatherEvents{f_name}.csv'\n",
    "\n",
    "\n",
    "'''This function extracts and return all the existing sequences, given a node as root node'''\n",
    "def iterativePatternChainFinder(root, _e, node_dict): \n",
    "    \n",
    "    finalSequences = []\n",
    "    seq_current = [[root]]\n",
    "    seq_next = []\n",
    "    \n",
    "    while len(seq_current)>0: \n",
    "                                                         \n",
    "        for s in seq_current:   \n",
    "            if s[-1] not in node_dict: continue\n",
    "            e = node_dict[s[-1]]\n",
    "            st = set()\n",
    "            st.update(s)\n",
    "            flag = True\n",
    "            \n",
    "            if len(e.childs) > 0:\n",
    "                for c in e.childs:\n",
    "                    if c not in node_dict: continue\n",
    "                    _c = node_dict[c]\n",
    "                    if _c.visited: continue\n",
    "                    _c.visited = True\n",
    "                    node_dict[c] = _c\n",
    "                    \n",
    "                    if c not in st:\n",
    "                        flag = False\n",
    "                        seq_next.append(s + [c])\n",
    "                                            \n",
    "            if flag or len(e.childs)==0:\n",
    "                seq = []\n",
    "                for _s in s: \n",
    "                    if _s not in node_dict: continue\n",
    "                    seq.append(node_dict[_s].refined_type + '_' + _s)                    \n",
    "                finalSequences.append(seq)\n",
    "                                                                                                        \n",
    "        seq_current = seq_next    \n",
    "        seq_next = []    \n",
    "        \n",
    "    return finalSequences, node_dict\n",
    "\n",
    "\n",
    "def findSequences(events):\n",
    "    new_events = list(set().union(*events[0]))\n",
    "    new_events = new_events + events[1]\n",
    "    zip_to_sequences = {}\n",
    "    \n",
    "    node_dict = {}\n",
    "    \n",
    "    for e in new_events:\n",
    "        node_dict[e.eventId] = node(e.type, e.refinedType, e.zipCode, e.childs, e.parents)\n",
    "    \n",
    "    for e in new_events:                \n",
    "        \n",
    "        if len(e.childs) == len(e.parents) == 0: #no child and parent\n",
    "            continue\n",
    "        if len(e.parents) > 0: #if some event has a parent, this means it is already processed or will be\n",
    "            continue\n",
    "        \n",
    "        seq = []\n",
    "        c_idx = []\n",
    "        idx = 0\n",
    "        for c in e.childs: #c is an event without parent\n",
    "            if c not in node_dict: continue\n",
    "            _c = node_dict[c] \n",
    "            if _c.visited: continue\n",
    "            _c.visited = True\n",
    "            node_dict[c] = _c\n",
    "            \n",
    "            seqSet, node_dict = iterativePatternChainFinder(c, e, node_dict)\n",
    "            \n",
    "            for s in seqSet:\n",
    "                seq.append(([e.refinedType + '_' + e.eventId] + s))\n",
    "                c_idx.append(idx)\n",
    "            idx += 1\n",
    "        \n",
    "        \n",
    "        idx = 0\n",
    "        for s in seq:\n",
    "            if e.type == 'T':\n",
    "                z = e.zipCode\n",
    "            else:\n",
    "                e_child_list = list(e.childs)\n",
    "                if e_child_list[c_idx[idx]] not in node_dict: continue\n",
    "                z = node_dict[e_child_list[c_idx[idx]]].zip\n",
    "                \n",
    "            seqs = []\n",
    "            if z in zip_to_sequences:\n",
    "                seqs = zip_to_sequences[z]\n",
    "            seqs.append(s)\n",
    "            zip_to_sequences[z] = seqs\n",
    "            \n",
    "            idx += 1\n",
    "    \n",
    "    final_list = []\n",
    "    for z in zip_to_sequences:\n",
    "        value = zip_to_sequences[z]\n",
    "        final_list.append((z,value))\n",
    "        \n",
    "    return final_list\n",
    "\n",
    "def createBasicUnorderedRootedTreeStructures(data):\n",
    "    zipCode = data[0]\n",
    "    if len(data)>1:\n",
    "        sequences = data[1]\n",
    "    else:\n",
    "        sequences = []\n",
    "    nodes = {}\n",
    "    roots = []\n",
    "    \n",
    "    for s in sequences:\n",
    "        for i in range(len(s)):\n",
    "            if s[i] in nodes: \n",
    "                n = nodes[s[i]]\n",
    "                if i < len(s)-1: \n",
    "                    n.childs.add(s[i+1])                        \n",
    "            else: \n",
    "                cSet = set()\n",
    "                if i < len(s)-1: cSet.add(s[i+1])\n",
    "                n = node2(s[i].split('_')[0], (-1 if (i-1)<0 else s[i-1]), cSet)\n",
    "                if n.parent == -1: roots.append(s[i]) \n",
    "\n",
    "            nodes[s[i]] = n     \n",
    "            \n",
    "    return [[zipCode] + [nodes] + [roots]]\n",
    "\n",
    "\n",
    "def createLabelToCode():\n",
    "    labelToCode = {}\n",
    "    \n",
    "    labelToCode['Snow-Light'] = '1'\n",
    "    labelToCode['Snow-Moderate'] = '1'\n",
    "    labelToCode['Snow-Heavy'] = '1'\n",
    "    \n",
    "    labelToCode['Rain-Light'] = '2'\n",
    "    labelToCode['Rain-Moderate'] = '2'\n",
    "    labelToCode['Rain-Heavy'] = '2'\n",
    "    \n",
    "    labelToCode['Construction'] = '3'\n",
    "    labelToCode['Construction-Other'] = '3'\n",
    "    labelToCode['Construction-Short'] = '3'\n",
    "    \n",
    "    labelToCode['Congestion'] = '4'\n",
    "    labelToCode['Congestion-Fast'] = '4'\n",
    "    labelToCode['Congestion-Moderate'] = '4'\n",
    "    labelToCode['Congestion-Slow'] = '4'\n",
    "    \n",
    "    labelToCode['Event-Short'] = '5'\n",
    "    labelToCode['Event-Long'] = '5'\n",
    "    labelToCode['Event'] = '5'\n",
    "    \n",
    "    labelToCode['Fog-Moderate'] = '6'\n",
    "    labelToCode['Fog-Severe'] = '6'\n",
    "    \n",
    "    labelToCode['Lane-Blocked'] = '7'\n",
    "    labelToCode['Cold-Severe'] = '8'\n",
    "    labelToCode['Other'] = '9'\n",
    "    labelToCode['Storm-Severe'] = '10'\n",
    "    labelToCode['Broken-Vehicle'] = '11'\n",
    "    labelToCode['Incident-Weather'] = '12'\n",
    "    labelToCode['Precipitation-UNK'] = '13'\n",
    "    labelToCode['Hail-Other'] = '14'\n",
    "    labelToCode['Incident-Other'] = '15'\n",
    "    labelToCode['Incident-Flow'] = '16'\n",
    "    labelToCode['Flow-Incident'] = '16'\n",
    "    labelToCode['Accident'] = '17'\n",
    "    \n",
    "    return labelToCode\n",
    "    \n",
    "def convertToTreePreOrderedDfsEncoding(data):\n",
    "    zipCode = data[0]\n",
    "    roots = data[2]\n",
    "    nodes = data[1]\n",
    "    encodings = []\n",
    "\n",
    "    for r in roots:\n",
    "        list = [r]\n",
    "        enc = labelToCode[nodes[r].label]\n",
    "        nodes[r].color = 'b'  # as our graph is a tree, we don't need three colors; thus, just use white (w) and black (b)\n",
    "        while len(list)>0:\n",
    "            n = list[len(list)-1]\n",
    "            flag = False                \n",
    "            for c in nodes[n].childs:\n",
    "                if nodes[c].color == 'w':\n",
    "                    flag = True\n",
    "                    nodes[c].color = 'b'\n",
    "                    list.append(c)\n",
    "\n",
    "                    #if nodes[c].label not in labelToCode: labelToCode[nodes[c].label] = str(len(labelToCode) + 1)\n",
    "                    enc += ' ' + labelToCode[nodes[c].label]\n",
    "                    break\n",
    "            if flag: continue\n",
    "            list.pop(len(list)-1)\n",
    "            if n not in roots:  enc += ' -1'\n",
    "\n",
    "        enc = str(len(encodings)+1) + ' ' + str(len(encodings)+1) + ' ' + str(len(enc.split(' '))) + ' ' + enc \n",
    "        encodings.append(enc)\n",
    "    \n",
    "    return [(zipCode, encodings)]\n",
    "\n",
    "\n",
    "def format_airport_row(r: str) -> str:\n",
    "    parts = r.split(',')\n",
    "    return (str(int(parts[0])),parts[1])\n",
    "\n",
    "print(\"\\nLoading Traffic and Weather events...\")\n",
    "input_rdd = sc.textFile(traffic_path).map(event_transformer)\n",
    "input_rdd2 = sc.textFile(weather_path).map(event_transformer)\n",
    "input_rdd3 = sc.textFile(airport_path).filter(lambda it: not it.startswith('Zip,')).map(format_airport_row)\n",
    "\n",
    "print(\"%d Traffic events\\n%d Weather events\" % (input_rdd.count(),input_rdd2.count()))\n",
    "\n",
    "traffic_rdd_groupByZip = input_rdd.map(lambda e: (e.zipCode, e)).groupByKey().mapValues(list)\n",
    "\n",
    "traffic_rdd_join = input_rdd3.join(traffic_rdd_groupByZip) #key = zipCode, value = (airportCode, [TrafficEvents])\n",
    "traffic_rdd_join_map = traffic_rdd_join.map(lambda it: (it[1][0],it[1][1])) #key = airportCode, value = [TrafficEvents]\n",
    "traffic_rdd_join_map_gbk = traffic_rdd_join_map.groupByKey().mapValues(list) #key = airportCode, value = [[TrafficEvents]] \n",
    "\n",
    "weather_rdd_groupByAirCode = input_rdd2.map(lambda e: (e.airportCode, e)).groupByKey().mapValues(list)\n",
    "traffic_weather_join = traffic_rdd_join_map_gbk.join(weather_rdd_groupByAirCode) #key = airportCode, value = ([[TrafficEvents]], [WeatherEvents])\n",
    "\n",
    "zip_to_sequences = traffic_weather_join.values().flatMap(lambda events: findSequences(events))\n",
    "\n",
    "labelToCode = createLabelToCode()\n",
    "\n",
    "zipToNodesRoots = zip_to_sequences.flatMap(createBasicUnorderedRootedTreeStructures)\n",
    "zipToEncodingRDD = zipToNodesRoots.flatMap(convertToTreePreOrderedDfsEncoding)\n",
    "\n",
    "print(f'\\n{CITY} zipCodes:\\n')\n",
    "print(zips_to_filter)\n",
    "\n",
    "zipToEncodingCity = zipToEncodingRDD.filter(lambda it: it[0] in zips_to_filter)\n",
    "\n",
    "\n",
    "### CODE TO WRITE ENCODING LIST ###\n",
    "w = open(f'encoding_lists/encodingList_TTR-{trTimeThresh}_{CITY}', 'w')\n",
    "for t in zipToEncodingCity.collect():\n",
    "    for tree in t[1]:\n",
    "        w.write(tree + '\\n')\n",
    "w.close()\n",
    "print(f'\\nencodingList_TTR-{trTimeThresh}_{CITY} has been created')\n",
    "\n",
    "print(\"\\nElapsed Time: %.2fs\" % (time.time()-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
