{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b7812c-b23e-40c2-897b-4f978b6c2989",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from conf import *\n",
    "import random\n",
    "import pyspark\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from sys import exit\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark import StorageLevel\n",
    "from operator import add\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from interval_tree import Interval, Node, IntervalTree\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "from pathlib import Path\n",
    "from event import event\n",
    "\n",
    "\n",
    "def event_transformer(line: str) -> event:\n",
    "    r = line.replace('\\r', '').replace('\\n', '').split(',')\n",
    "    if r[1] == 'W':\n",
    "        e = event(eventId=r[0], type='W', refinedType=r[2], startTime=r[3], endTime=r[4], locationLat=0, locationLng=0,  \n",
    "                          distance=0, airportCode=r[8], number=0, street='NA', side='NA', city='NA', county='NA', state='NA', \n",
    "                          zipCode='NA', childs=set(), parents=set())\n",
    "    else:\n",
    "        e = event(eventId=r[0], type='T', refinedType=r[2], startTime=r[3], endTime=r[4], locationLat=float(r[5]), \n",
    "                          locationLng=float(r[6]), distance=float(r[7]), airportCode=r[15], number=(0 if r[9]=='N/A' or r[9]=='' else int(r[9])), \n",
    "                          street=r[10], side=r[11], city=r[12], county=r[13], state=r[14], zipCode=r[15], childs=set(), parents=set())\n",
    "    return e\n",
    "\n",
    "def str_transformer(e: event) -> str:\n",
    "    child, parents = list(e.childs), list(e.parents)\n",
    "    child_str = ';'.join(child) if len(child) > 0 else ''\n",
    "    parent_str = ';'.join(parents) if len(parents) > 0 else ''\n",
    "    if e.type == 'W':\n",
    "        res = f'{e.eventId},{e.type},{e.refinedType},{e.startTime},{e.endTime},N/A,N/A,N/A,{e.airportCode},N/A,N/A,N/A,N/A,N/A,N/A,N/A,{child_str},{parent_str}'\n",
    "\n",
    "    else:\n",
    "        res = f'{e.eventId},{e.type},{e.refinedType},{e.startTime},{e.endTime},{e.locationLat},{e.locationLng},{e.distance},{e.airportCode},{e.number},{e.street},{e.side},{e.city},{e.county},{e.state},{e.zipCode},{child_str},{parent_str}'\n",
    "\n",
    "    return res\n",
    "    \n",
    "\n",
    "\n",
    "sc.stop()\n",
    "# Create new config\n",
    "conf = pyspark.SparkConf().setAll([(\"spark.driver.maxResultSize\", '16g'), ('spark.executor.memoryOverhead', '16g'), ('spark.executor.memory', '16g')])\n",
    "\n",
    "SparkSession.builder.config(conf=conf)\n",
    "spark = SparkSession.builder.appName('test_03_11_1').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.addPyFile('file:/event.py')\n",
    "sc.addPyFile('file:/interval_tree.py')\n",
    "storage_level = pyspark.StorageLevel.MEMORY_AND_DISK\n",
    "sc.setLogLevel(\"OFF\")\n",
    "\n",
    "#print(spark.sparkContext.getConf().getAll())\n",
    "\n",
    "#SUBSET = True # False if you are the entire datasets, True otherwise\n",
    "if SUBSET:\n",
    "    f_name = f'_{CITY}'\n",
    "    res = input(f'You are using a SUBSET ({CITY}). Are you sure? [y/n]:')\n",
    "else:\n",
    "    res = input('You are using ENTIRE DATASET. Are you sure? [y/n]:')\n",
    "    f_name = ''\n",
    "\n",
    "if(res != 'y'):\n",
    "    exit()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "traffic_path = f'file:/datasets/TrafficEvents_Aug16_Dec20_Publish{f_name}.csv'\n",
    "weather_path = f'file:/datasets/WeatherEvents_Aug16_Dec20_Publish{f_name}.csv'\n",
    "\n",
    "def format_airport_row(r: str) -> str:\n",
    "    parts = r.split(',')\n",
    "    return (str(int(parts[0])),parts[1])\n",
    "\n",
    "start_phase_1 = time.time()\n",
    "print(\"\\nLoading Traffic and Weather events...\")\n",
    "input_rdd = sc.textFile(traffic_path)\n",
    "input_rdd2 = sc.textFile(weather_path)\n",
    "input_rdd3 = sc.textFile(airport_path).filter(lambda it: not it.startswith('Zip,')).map(format_airport_row)\n",
    "\n",
    "print(\"%d Traffic events\\n%d Weather events\" % (input_rdd.count(),input_rdd2.count()))\n",
    "print(\"Elapsed Time: %.2fs\" % (time.time()-start_phase_1))\n",
    "\n",
    "def format_event_row(r: str, i: int) -> str:\n",
    "    parts = r.split(',')\n",
    "    if r.startswith('T'):\n",
    "        return 'T-' + str(i+1) + ',T,' + parts[1] + ',' + parts[5] + ',' + parts[6] + ',' + \\\n",
    "    parts[8] + ',' + parts[9] + ',' + parts[10] + ',' + parts[11] + ',' + parts[12] + ',' + \\\n",
    "    parts[13] + ',' + parts[14] + ',' + parts[15] + ',' + parts[16] + ',' + parts[17] + ',' + parts[18]\n",
    "    \n",
    "    else:\n",
    "        return 'W-' + str(i+1) + ',W,' + parts[1] + '-' + parts[2] + ',' + parts[3] + ',' + \\\n",
    "    parts[4] + ',N/A,N/A,N/A,' + parts[8] + ',N/A,N/A,N/A,N/A,N/A,N/A,N/A'\n",
    "\n",
    "\n",
    "start_phase_2 = time.time()\n",
    "print(\"\\nRemoving Traffic events without Zip Code and Weather events without airportCode...\")\n",
    "traffic_rdd_map = input_rdd.filter(lambda it: not it.startswith('EventId,') and not it.split(',')[18]=='').zipWithIndex().map(lambda it: (it[0].split(',')[18], format_event_row(it[0], it[1])))\n",
    "weather_rdd_map = input_rdd2.filter(lambda it: not it.startswith('EventId,') and not it.split(',')[8]=='').zipWithIndex().map(lambda it: (it[0].split(',')[8], format_event_row(it[0], it[1])))\n",
    "\n",
    "t1_count = traffic_rdd_map.count()\n",
    "w1_count = weather_rdd_map.count()\n",
    "print(\"%d Traffic events (after first cleaning)\\n%d Weather events (after first cleaning)\" % (t1_count,w1_count))\n",
    "print(\"Elapsed Time: %.2fs\" % (time.time()-start_phase_2))\n",
    "      \n",
    "traffic_rdd_gbk = traffic_rdd_map.groupByKey().mapValues(list)\n",
    "weather_rdd_gbk = weather_rdd_map.groupByKey().mapValues(list)\n",
    "\n",
    "\n",
    "distanceThresh = 0.2\n",
    "dist_unit_sphere = distanceThresh / 3958.7564\n",
    "\n",
    "def integrateSimilarWeatherEvents(events):\n",
    "    coords = []\n",
    "    event_obj_ids_dict = {}\n",
    "    \n",
    "    for num, e in enumerate(events):\n",
    "        e_obj = event_transformer(e)\n",
    "        event_obj_ids_dict[num] = e_obj\n",
    "        coords.append([e_obj.locationLat, e_obj.locationLng])\n",
    "    \n",
    "    coords = np.array(coords)\n",
    "    coords = np.deg2rad(coords)    \n",
    "    bt = BallTree(coords, metric='haversine')\n",
    "    neighs_idx = bt.query_radius(coords, r=dist_unit_sphere) \n",
    "    \n",
    "    new_events = {}\n",
    "        \n",
    "    for idx, array in enumerate(neighs_idx):\n",
    "        if len(array)==1:\n",
    "            ev = event_obj_ids_dict[array[0]]\n",
    "            if ev.toBeMerged:\n",
    "                continue\n",
    "            new_events[ev.eventId] = ev\n",
    "            continue\n",
    "        \n",
    "        for ev in array:\n",
    "            \n",
    "            if idx==ev:\n",
    "                continue\n",
    "            \n",
    "            e1 = event_obj_ids_dict[idx]\n",
    "            \n",
    "            if e1.toBeMerged:\n",
    "                continue\n",
    "            \n",
    "            e2 = event_obj_ids_dict[ev]\n",
    "                            \n",
    "            if e1.refinedType == e2.refinedType:\n",
    "                timeDiff = max((e1.startTime - e2.endTime).total_seconds(), (e2.startTime - e1.endTime).total_seconds())\n",
    "                if 'snow' in e1.refinedType: th = wTimeThreshs['snow']\n",
    "                elif 'rain' in e1.refinedType: th = wTimeThreshs['rain']\n",
    "                else: th = wTimeThreshs['default']\n",
    "                if timeDiff < (th*60):\n",
    "                    e1.startTime = min(e1.startTime, e2.startTime)\n",
    "                    e1.endTime = max(e1.endTime, e2.endTime)                              \n",
    "                    e2.toBeMerged = True \n",
    "                \n",
    "            new_events[e1.eventId] = e1\n",
    "        \n",
    "    return list(new_events.values())\n",
    "    \n",
    "def integrateSimilarTrafficIncidents(events):\n",
    "        \n",
    "    _trTimeThresh = 5\n",
    "    coords = []\n",
    "    event_obj_ids_dict = {}\n",
    "    \n",
    "    for num, e in enumerate(events):\n",
    "        e_obj = event_transformer(e)\n",
    "        event_obj_ids_dict[num] = e_obj\n",
    "        coords.append([e_obj.locationLat, e_obj.locationLng])\n",
    "        \n",
    "    coords = np.array(coords)\n",
    "    coords = np.deg2rad(coords)    \n",
    "    bt = BallTree(coords, metric='haversine')\n",
    "    neighs_idx = bt.query_radius(coords, r=dist_unit_sphere) \n",
    "        \n",
    "    new_events = {}\n",
    "        \n",
    "    for idx, array in enumerate(neighs_idx):\n",
    "        if len(array)==1:\n",
    "            ev = event_obj_ids_dict[array[0]]\n",
    "            if ev.toBeMerged:\n",
    "                continue\n",
    "            new_events[ev.eventId] = ev\n",
    "            continue\n",
    "        \n",
    "        for ev in array:\n",
    "            \n",
    "            if idx==ev:\n",
    "                continue\n",
    "            \n",
    "            e1 = event_obj_ids_dict[idx]\n",
    "            \n",
    "            if e1.toBeMerged:\n",
    "                continue\n",
    "            \n",
    "            e2 = event_obj_ids_dict[ev]\n",
    "                            \n",
    "            timeDiff = abs((e1.startTime - e2.startTime).total_seconds())\n",
    "            \n",
    "            if timeDiff < (_trTimeThresh*60) and e1.refinedType==e2.refinedType and 'Congestion' in e1.refinedType and e1.street==e2.street and e1.side==e2.side:\n",
    "                e1.startTime = min(e1.startTime, e2.startTime)\n",
    "                e1.endTime = max(e1.endTime, e2.endTime)\n",
    "                e1.distance = max(e1.distance, e2.distance)  \n",
    "                e2.toBeMerged = True\n",
    "                \n",
    "            new_events[e1.eventId] = e1\n",
    "        \n",
    "    return list(new_events.values())\n",
    "\n",
    "def findChildParents(data):\n",
    "    weatherEvents = data[1]\n",
    "    trafficEvents = data[0]\n",
    "    coords = []\n",
    "    traffic_ids_dict = {}\n",
    "    weather_ids_dict = {}\n",
    "    interval_tree = IntervalTree()\n",
    "        \n",
    "    trafficEvents = list(set().union(*trafficEvents))\n",
    "    \n",
    "    for num, e in enumerate(trafficEvents):\n",
    "        traffic_ids_dict[num] = e\n",
    "        coords.append([e.locationLat, e.locationLng])\n",
    "        \n",
    "    coords = np.array(coords)\n",
    "    coords = np.deg2rad(coords)    \n",
    "    bt = BallTree(coords, metric='haversine')\n",
    "    neighs_idx = bt.query_radius(coords, r=dist_unit_sphere) \n",
    "    \n",
    "    for e in weatherEvents:\n",
    "        weather_ids_dict[e.eventId] = e\n",
    "        try: th = wTimeThreshs[e.refinedType]\n",
    "        except: th = 5\n",
    "\n",
    "        interval = Interval(e.startTime + timedelta(minutes=5), e.endTime + timedelta(minutes=th))\n",
    "        interval_tree.add(interval, event_id=e.eventId)\n",
    "\n",
    "    for idx, array in enumerate(neighs_idx):\n",
    "        e1 = traffic_ids_dict[idx]\n",
    "        for ev in array:\n",
    "            \n",
    "            if idx == ev:\n",
    "                continue\n",
    "            \n",
    "            e2 = traffic_ids_dict[ev]\n",
    "                            \n",
    "            if e1.street != e2.street or \\\n",
    "                    e1.side != e2.side or \\\n",
    "                    (e1.endTime + timedelta(minutes=trTimeThresh)) < e2.startTime or \\\n",
    "                    e1.startTime > (e2.endTime + timedelta(minutes=trTimeThresh)):\n",
    "                    continue\n",
    "\n",
    "            if e1.startTime < e2.startTime:\n",
    "                e1.childs.add(e2.eventId)\n",
    "                e2.parents.add(e1.eventId)\n",
    "            else:           \n",
    "                e2.childs.add(e1.eventId)\n",
    "                e1.parents.add(e2.eventId)\n",
    "            \n",
    "        incident_start_time = e1.startTime\n",
    "        events_of_interests = interval_tree.query_point(incident_start_time)\n",
    "            \n",
    "        for ed in events_of_interests:\n",
    "            e_id = ed['event_id']\n",
    "            e1.parents.add(e_id)\n",
    "            weather_ids_dict[e_id].childs.add(e1.eventId)\n",
    "    \n",
    "    res = list(traffic_ids_dict.values()) + list(weather_ids_dict.values())\n",
    "    return res\n",
    "    \n",
    "start_phase_3 = time.time()\n",
    "print(\"\\nChecking for similar Traffic events...\")\n",
    "traffic_flatmap = traffic_rdd_gbk.values().flatMap(lambda events: integrateSimilarTrafficIncidents(events))\n",
    "t2_count = traffic_flatmap.count()\n",
    "print(\"Have found %d similar pairs of traffic incidents\" % (t1_count-t2_count))\n",
    "print(\"%d Traffic events (after similarity check)\" % t2_count)\n",
    "print(\"Elapsed Time: %.2fs\" % (time.time()-start_phase_3))\n",
    "\n",
    "traffic_rdd_map_v2 = traffic_flatmap.map(lambda it: (it.zipCode, it))\n",
    "traffic_rdd_gbk_v2 = traffic_rdd_map_v2.groupByKey().mapValues(list) #key = zipCode, value = [TrafficEvents]\n",
    "\n",
    "traffic_rdd_join = input_rdd3.join(traffic_rdd_gbk_v2) #key = zipCode, value = (airportCode, [TrafficEvents])\n",
    "traffic_rdd_join_map = traffic_rdd_join.map(lambda it: (it[1][0],it[1][1])) #key = airportCode, value = [TrafficEvents]\n",
    "traffic_rdd_join_map_gbk = traffic_rdd_join_map.groupByKey().mapValues(list) #key = airportCode, value = [[TrafficEvents]]\n",
    "\n",
    "wTimeThreshs = {'rain':15, 'snow':30, 'default':10}\n",
    "start_phase_4 = time.time()\n",
    "print(\"\\nChecking for similar Weather events...\")\n",
    "weather_flatmap = weather_rdd_gbk.values().flatMap(lambda events: integrateSimilarWeatherEvents(events))\n",
    "w2_count = weather_flatmap.count()\n",
    "print(\"Have found %d similar pairs of weather events\" % (w1_count-w2_count))\n",
    "print(\"%d Weather events (after similarity check)\" % w2_count)\n",
    "print(\"Elapsed Time: %.2fs\" % (time.time()-start_phase_4))\n",
    "\n",
    "'''\n",
    "#### SAVING TO FILE ALL EVENTS DISTINCT\n",
    "\n",
    "print(\"\\nSaving to file All Events Distinct...\")\n",
    "traffic_flatmap_912 = traffic_flatmap.map(str_transformer)\n",
    "weather_flatmap_912 = weather_flatmap.map(str_transformer)\n",
    "    \n",
    "traffic_flatmap_912.saveAsTextFile(f'file:/datasets/Output_Traffic_912')\n",
    "weather_flatmap_912.saveAsTextFile(f'file:/datasets/Output_Weather_912')\n",
    "\n",
    "#############\n",
    "'''\n",
    "\n",
    "weather_rdd_map_v2 = weather_flatmap.map(lambda it: (it.airportCode, it))\n",
    "weather_rdd_gbk_v2 = weather_rdd_map_v2.groupByKey().mapValues(list) #key = airportCode, value = [WeatherEvents]\n",
    "\n",
    "traffic_weather_join = traffic_rdd_join_map_gbk.join(weather_rdd_gbk_v2) #key = airportCode, value = ([[TrafficEvents]], [WeatherEvents])\n",
    "\n",
    "start_phase_5 = time.time()\n",
    "print(f'\\nChecking for Childs and Parents events (trTimeThresh={trTimeThresh})...')\n",
    "traffic_weather_join_map_cp = traffic_weather_join.values().flatMap(lambda data: findChildParents(data)) #value = ([TrafficEvents]+[WeatherEvents])\n",
    "\n",
    "new_traffic_rdd = traffic_weather_join_map_cp.filter(lambda it: it.type=='T').map(lambda it: (it.eventId, it))\n",
    "new_weather_rdd = traffic_weather_join_map_cp.filter(lambda it: it.type=='W').map(lambda it: (it.eventId, it))\n",
    "\n",
    "traffic_flatmap_tmp = traffic_flatmap.map(lambda it: (it.eventId, it))\n",
    "tmp_traffic_rdd = traffic_flatmap_tmp.subtractByKey(new_traffic_rdd)\n",
    "final_traffic_rdd = tmp_traffic_rdd.union(new_traffic_rdd)\n",
    "\n",
    "weather_flatmap_tmp = weather_flatmap.map(lambda it: (it.eventId, it))\n",
    "tmp_weather_rdd = weather_flatmap_tmp.subtractByKey(new_weather_rdd)\n",
    "final_weather_rdd = tmp_weather_rdd.union(new_weather_rdd)\n",
    "print(\"%d New Traffic events\\n%d New Weather events\" % (final_traffic_rdd.count(),final_weather_rdd.count()))\n",
    "print(\"Elapsed Time: %.2fs\" % (time.time()-start_phase_5))\n",
    "\n",
    "print(\"%d Zip Codes\\n%d Airport Codes\" % (traffic_rdd_gbk_v2.count(),weather_rdd_gbk_v2.count()))\n",
    "print(\"\\nTotal Elapsed Time: %.2fs\" % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8f8bf-0164-4f2e-b294-cdafcebd71fd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "##### Code to save both datasets to file, REMEMBER to set the SUBSET flag #####\n",
    "start = time.time()\n",
    "print(\"\\nSaving to file...\")\n",
    "final_traffic_rdd_v2 = final_traffic_rdd.map(lambda it: str_transformer(it[1]))\n",
    "final_weather_rdd_v2 = final_weather_rdd.map(lambda it: str_transformer(it[1]))\n",
    "\n",
    "f_name += f'_TTR-{trTimeThresh}'\n",
    "    \n",
    "final_traffic_rdd_v2.saveAsTextFile(f'file:/datasets/Output_Traffic{f_name}')\n",
    "final_weather_rdd_v2.saveAsTextFile(f'file:/datasets/Output_Weather{f_name}')\n",
    "\n",
    "subprocess.call(f'cat Output_Traffic{f_name}/part* > TrafficEvents{f_name}.csv', shell=True, cwd='datasets/')\n",
    "subprocess.call(f'cat Output_Weather{f_name}/part* > WeatherEvents{f_name}.csv', shell=True, cwd='datasets/')\n",
    "subprocess.call(f'rm -r Output_Traffic{f_name}', shell=True, cwd='datasets/')\n",
    "subprocess.call(f'rm -r Output_Weather{f_name}', shell=True, cwd='datasets/')\n",
    "\n",
    "print(\"Elapsed Time: %.2fs\" % (time.time()-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
